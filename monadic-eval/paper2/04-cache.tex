\section{Detecting Cycles with a Cache}\label{s:cache}

At this point, the interpreter obtained by linking together ⸨monad^@⸩, ⸨δ^@⸩,
⸨alloc^@⸩ and ⸨store-nd@⸩ components will only ever visit a finite number of
configurations for a given program. A configuration (⸨ς⸩) consists of an
expression (⸨e⸩), environment (⸨ρ⸩) and store (⸨σ⸩). This configuration is
finite because: expressions are finite in the given program; environments are
maps from variables (again, finite in the program) to addresses; the addresses
are finite thanks to ⸨alloc^⸩; the store maps addresses to sets of values; base
values are abstracted to a finite set by ⸨δ^⸩; and closures consist of an
expression and environment, which are both finite.

Although the interpreter will only ever see a finite set of inputs, it
\emph{doesn't know it}.  A simple loop will cause the interpreter to diverge:
ℑ⁅
¦ > (rec f (λ (x) (f x)) (f 0))
ℑ,
¦ timeout
ℑ⁆
To solve this problem, we introduce a \emph{cache} (⸨¢⸢in⸣⸩) as input to the
algorithm, which maps from configurations (⸨ς⸩) to sets of value-and-store
pairs (⸨v×σ⸩). When a configuration is reached for the second time, rather than
re-evaluating the expression and entering an infinite loop, the result is
looked up from ⸨¢⸢in⸣⸩, which acts as an oracle. It is important that the cache
is used co-inductively: it is only safe to use ⸨¢⸢in⸣⸩ as an oracle so long as
some progress has been made first. 

The results of evaluation are then stored in an output cache (⸨¢⸢out⸣⸩), which
after the end of evaluation is “more defined” than the input cache (⸨¢⸢in⸣⸩),
again following a co-inductive argument. The least fixed-point of ⸨¢⁺⸩ of an
evaluator which transforms an oracle ⸨¢⸢in⸣⸩ and outputs a more defined oracle
⸨¢⸢out⸣⸩ is then a sound approximation of the program, because it
over-approximate all finite-number of unrollings of the unfixed evaluator. We
formalize this co-inductive process in Section~\ref{s:formalism} and prove it
sound; in this section we instead focus on the intuition and implementation for
the algorithm.

\begin{figure} %{-{
\rfloat{⸨monad-cache@⸩}
\begin{flalign*}
            & 𝔥⸨(define-monad (⸩\!\up{𝔥⸨ReaderT⸩}⸢env⸣\ 𝔥⸨(⸩\!\up{𝔥⸨FailT⸩}⸢errors⸣
& \\[-0.5em]& ␣␣𝔥⸨(⸩\!\up{𝔥⸨StateT⸩}⸢store⸣\ 𝔥⸨(⸩\!\up{𝔥⸨NondetT⸩}⸢branching⸣\ 𝔥⸨(⸩\!\up{𝔥⸨ReaderT⸩}⸢in-cache⸣\ 𝔥⸨(⸩\!\up{𝔥⸨StateT+⸩}⸢out-cache⸣\ 𝔥⸨ID)))))))⸩
& \end{flalign*}
\figskip\rfloat{⸨ev-cache@⸩}
\begin{lstlisting}
¦ (define (((ev-cache ev₀) ev) e)
¦   (do ρ ← ask-env
¦       σ ← get-store
¦       ς ≔ (list e ρ σ)
¦       ¢⸢out⸣ ← get-cache-out
¦       (if (∈ ς ¢⸢out⸣)
¦           (for/monad+ ([v×σ (¢⸢out⸣ ς)])
¦             (do (put-store (cdr v×σ))
¦                 (return (car v×σ))))
¦           (do ¢⸢in⸣ ← ask-cache-in
¦               v×σ₀ ≔ (if (∈ ς ¢⸢in⸣) (¢⸢in⸣ ς) ∅)
¦               (put-cache-out (¢⸢out⸣ ς v×σ₀))
¦               v  ← ((ev₀ ev) e)
¦               σ′ ← get-store
¦               v×σ′ ≔ (cons v σ′)
¦               (update-cache-out
¦                 (λ (¢⸢out⸣) 
¦                   (¢⸢out⸣ ς (set-add (¢⸢out⸣ ς) v×σ′))))
¦               (return v)))))
\end{lstlisting}
\caption{Co-inductive Caching Algorithm}
\label{f:caching}
\end{figure} %}-}

The co-inductive caching algorithm is shown in Figure~\ref{f:caching}, along
with the monad transformer stack ⸨monad-cache@⸩ which has two new components:
⸨ReaderT⸩ for the input cache ⸨¢⸢in⸣⸩, and ⸨StateT+⸩ for the output cache
⸨¢⸢out⸣⸩. We use a ⸨StateT+⸩ instead of ⸨WriterT⸩ monad transformer in the
output cache so it can double as tracking the set of seen states. The ⸨+⸩ in
⸨StateT+⸩ signifies that caches for multiple non-deterministic branches will be
merged automatically, producing a set of results and a single cache, rather
than a set of results paired with individual caches.

In the algorithm, when a configuration ⸨ς⸩ is first encountered, we place an
entry in the output cache mapping ⸨ς⸩ to «𝔥⸨(¢⸢in⸣⸩\ 𝔥⸨ς)⸩», which is the
“oracle” result. Also, whenever we finish computing the result ⸨v×σ⸩ of
evaluating a configuration ⸨ς⸩, we place an entry in the output cache mapping
⸨ς⸩ to ⸨v×σ′⸩. Finally, whenever we reach a configuration ⸨ς⸩ for which a
mapping in the output cache exists, we use it immediately, ⸨return⸩ing each
result using the ⸨for/monad+⸩ iterator. Therefore, every “cache hit” on
⸨¢⸢out⸣⸩ is in one of two possible states: 1) we have already seen the
configuration, and the result is the oracle result, as desired; or 2) we have
already computed the “improved” result (w.r.t. the oracle), and need not
recompute it.

To compute the least fixed-point ⸨¢⁺⸩ for the evaluator ⸨ev-cache⸩ we perform a
standard Kleene fixpoint iteration starting from the empty map, the bottom
element for the cache, as shown in Figure~\ref{f:fixing}.

The algorithm runs the caching evaluator ⸨eval⸩ on the given program ⸨e⸩ from
the initial environment and store. This is done inside of ⸨mlfp⸩, a monadic
least fixed-point finder. After finding the least fixed-point, the final values
and store for the initial configuration ⸨ς⸩ are extracted and returned.

Termination of the least fixed-point is justified by the monotonicity of the
evaluator (it always returns an “improved” oracle), and the finite domain of
the cache, which maps abstract configurations to pairs of values and stores,
all of which are finite.

\begin{figure} %{-{
\rfloat{⸨fix-cache@⸩}
\begin{lstlisting}
¦ (define ((fix-cache eval) e)  
¦   (do ρ ← ask-env
¦       σ ← get-store
¦       ς ≔ (list e ρ σ)
¦       ¢⁺ ← (mlfp (λ (¢) 
¦              (do (put-cache-out ∅-map)
¦                  (put-store σ)
¦                  (local-cache-in ¢ (eval e))
¦                  get-cache-out)))
¦       (for/monad+ ([v×σ (¢⁺ ς)])
¦         (do (put-store (cdr v×σ))
¦             (return (car v×σ))))))
¦ (define (mlfp f)
¦   (let loop ([x ∅-map])
¦     (do x′ ← (f x)
¦         (if (equal? x′ x)
¦             (return x)
¦             (loop x′)))))
\end{lstlisting}
\caption{Finding Fixpoints in the Cache}
\label{f:fixing}
\end{figure} %}-}

With these peices in place we construct a complete interpreter:
\begin{lstlisting}
¦ (define (eval e)
¦   (mrun ((fix-cache (fix (ev-cache ev))) e)))
\end{lstlisting}
When linked with ⸨δ^⸩ and ⸨alloc^⸩, this interpreter is
a computable and sound abstraction of the original definitional interpreter:
ℑ⁅
¦ > (rec f (λ (x) (f x))
¦     (f 0))
ℑ,
¦ '()
ℑ;
¦ > (rec f (λ (n) (if0 n 1 (* n (f (sub1 n)))))
¦     (fact 5))
ℑ,
¦ '(N)
ℑ;
¦ > (rec f (λ (x) (if0 x 0 (if0 (f (sub1 x)) 2 3)))
¦      (f (add1 0)))
ℑ,
¦ '(0 2 3)
ℑ⁆

% Let us now take stock of what we've got.
% We use the following monad stack, which adds a ``cache'' component,
% which will be a finite map from states to sets of values:
% \begin{lstlisting}
% ¦ (ReaderT (FailT (StateT (NondetT (StateT+ ID)))))
% \end{lstlisting}
% 
% The ⸨StateT+⸩ monad transformer provides operations
% ⸨get-¢⸩ and ⸨update-¢›⸩ for getting and updating the
% cache, respectively. It joins its finite maps by union of the range
% when ⸨mplus⸩ is called, because it cannot defer to an
% underlying monoid as the outer ⸨StateT⸩ does with
% ⸨NondetT⸩.
% 
% Figure~\ref{f:ev-cache0} gives an ⸨ev⸩-wrapper that interposes
% itself on each recursive call to do the following steps:
% \begin{displayquote}
% Check if the current state is in the cache.  If it's in the cache, return all
% the results given in the cache.  If it's not, set the cache for the current
% state to the empty set, evaluate the expression, add the resulting value to the
% cache for the state, and return the result.
% \end{displayquote}
% 
% We can now define an evaluation function that mixes in ⸨ev-cache⸩:
% \begin{lstlisting}
% ¦ (define (eval e)
% ¦   (mrun ((fix (ev-cache ev)) e)))
% \end{lstlisting}
% 
% If we were to link this together with ⸨alloc@⸩ and ⸨δ@⸩, we'd obtain a concrete
% interpreter that either 1) produces the empty set because it encountered a
% loop, 2) produces a singleton result, or 3) diverges because it encounters an
% infinite set of states.  But if we were to link this together with ⸨alloc^@⸩
% and ⸨δ^@⸩, we'd obtain an abstract interpreter that is \emph{total}: it
% terminates on all inputs.
% 
% To see why this, observe that for a given program there only a finite set of
% possible caches.  We have already seen that there are a finite set of states
% and values, so it follows that there are only a finite set of maps from states
% to sets of values.  Now notice that on each recursive call, either the state is
% in the cache and it returns immediately, or the cache grows.  So programs
% simply cannot run forever because that would imply the cache would grow
% forever.
% 
% It should be easy to see that if evaluating a state ⸨ς⸩ requires recursively
% evaluating that same state, it will now produce the empty set since the cache
% will be updated to map ⸨ς⸩ to ⸨∅⸩ before proceeding to the sub-expressions.
% 
% We can now see that the caching abstract interpreter halts on programs that
% loop (for simplicity, the cache and store are omitted from the printed
% results):
% ℑ⁅
% ¦ > (rec f (λ (x) (f x)) (f 0))
% ℑ,
% ¦ '()
% ℑ⁆
% This accomplishes the goal of terminating on this example, and it even
% gives the right answer---the empty set---since this program produces
% no results.
% 
% It also works for recursive functions that terminate in the concrete,
% but have loops once abstracted:
% ℑ⁅
% ¦ > (rec fact (λ (n)
% ¦              (if0 n 1 (* n (fact (sub1 n)))))
% ¦     (fact 5))
% ℑ,
% ¦ '(N)
% ℑ⁆
% 
% It may seem we've accomplished our goal of making a sound and
% decidable abstract interpreter.  However this approach is broken in
% general: it is not sound in the presence of abstraction.  The problem
% here is that when the interpreter reaches ``the same'' state it has
% seen before, what we mean by ``the same'' in the presence of
% abstraction is subtle.  For example, imagine evaluating a function
% application of some function ⸨f⸩ to an abstract value
% ⸨'N⸩.  Suppose in evaluating this application we encounter
% another application of ⸨f⸩ to ⸨'N⸩.  Is it the same
% application?  Well, yes and no.  It is the same \emph{abstract} state,
% however the abstract state stands for a set of concrete states; in
% this case, the application of ⸨f⸩ to all numbers.  So there are
% states stood for in the abstraction that are equal \emph{and} not
% equal.  In other words, in the presence of abstraction, when a loop is
% detected, there \emph{may} be a loop in the concrete interpretation.
% Our naive loop detection set-up however is assuming there \emph{must}
% be a loop.
% 
% We can demonstrate the problem with a simple counter-example to
% soundness:
% ℑ⁅
% ¦ > (rec f (λ (x) 
% ¦            (if0 x 0 (if0 (f (sub1 x)) 2 3)))
% ¦      (f (add1 0)))
% ℑ,
% ¦ '(0)
% ℑ⁆
% 
% Concretely, this program returns ⸨2⸩, however with the
% combination of loop detection and abstraction, the abstract
% interpreter determines that this program produces ⸨0⸩, which is
% clearly unsound.
